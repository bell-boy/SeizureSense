{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torcheval.metrics as metrics\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from jaxtyping import Float\n",
    "from chbmit.chbmit import FilteredCMP\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "  def __init__(self, ):\n",
    "    super().__init__()\n",
    "    self.shared_layers = nn.Sequential(nn.Conv2d(23, 96, (11, 11), 4),\n",
    "                                       nn.MaxPool2d(2),\n",
    "                                       nn.Conv2d(96, 384, (5, 5), padding='same'), \n",
    "                                       nn.MaxPool2d(2),\n",
    "                                       nn.Conv2d(384, 384, (3, 3), padding='same'),\n",
    "                                       nn.Conv2d(384, 384, (3, 3), padding='same'),\n",
    "                                       nn.Conv2d(384, 256, (3, 3), padding='same'),\n",
    "                                       nn.MaxPool2d(2),\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Linear(12544, 4096),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(4096, 4096),\n",
    "    )\n",
    "    self.classification_head = nn.Sequential(\n",
    "      nn.Linear(4096, 1000),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1000, 1000),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1000, 2)\n",
    "    )\n",
    "\n",
    "    self.regression_head = nn.Sequential(\n",
    "      nn.Linear(4096, 1000),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1000, 1000),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1000, 1)\n",
    "    )\n",
    "\n",
    "    self.variance_head = nn.Sequential(\n",
    "      nn.Linear(4096, 1000),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1000, 1000),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(1000, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, data): \n",
    "    shared_output = self.shared_layers(data)\n",
    "    return (self.classification_head(shared_output), self.regression_head(shared_output), self.variance_head(shared_output.detach()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHTrainer():\n",
    "  def __init__(self,data_path: str, batch_size: int, learing_rate: float, n_fft: int):\n",
    "    self.init_model()\n",
    "    self.global_step = 0\n",
    "    self.dataset = FilteredCMP(60, 10, data_path, sop=3600, sph=0, regression=True)\n",
    "    self.train_data, self.val_data = random_split(self.dataset, [.8, .2])\n",
    "    self.DataLoader = DataLoader(self.train_data, batch_size=batch_size)\n",
    "    self.ValLoader = DataLoader(self.val_data, batch_size=batch_size)\n",
    "    self.optim = torch.optim.AdamW(self.model.parameters(), lr=learing_rate)\n",
    "    self.init_class_weights()\n",
    "    self.classification_critierion = nn.CrossEntropyLoss()\n",
    "    self.writer = SummaryWriter()\n",
    "\n",
    "  def init_model(self,):\n",
    "    self.model = MultiHead()\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def init_class_weights(self,):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for data, label in tqdm(self.DataLoader, leave=False):\n",
    "      data = self.process(data)\n",
    "      pred = self.model(data)[0]\n",
    "\n",
    "      label[label > 0] = 1\n",
    "      label = label.long()\n",
    "      pos += label[label == 1].size(-1)\n",
    "      neg += label[label == 0].size(-1)\n",
    "    \n",
    "    self.classification_critierion.weight = [(pos + neg) / (2 * neg), (pos + neg) / (2 * pos)]\n",
    "\n",
    "  def process(self, data):\n",
    "    batch_size = data.size(0)\n",
    "    data = data.reshape(batch_size * 23, -1)\n",
    "    data = torch.stft(data, n_fft=128, window=torch.hann_window(128), return_complex=False)\n",
    "    data = torch.sqrt((data[:, :, :, 0] ** 2 + data[:, :, :, 1] ** 2))\n",
    "    data = data.reshape(batch_size, 23, -1, data.size(-1))\n",
    "    data = F.interpolate(data, (256, 256))\n",
    "    return data\n",
    "\n",
    "  def train_epoch_classification(self):\n",
    "    for data, label in tqdm(self.DataLoader, leave=False,):\n",
    "      data = self.process(data)\n",
    "      pred = self.model(data)[0] \n",
    "\n",
    "      label[label != float('inf')] = 1\n",
    "      label[label == float('inf')] = 0\n",
    "      label = label.long()\n",
    "      loss = self.classification_critierion(pred, label)\n",
    "      self.writer.add_scalar('loss/train_class', loss, self.global_step)\n",
    "\n",
    "      self.optim.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optim.step()\n",
    "      self.global_step += 1\n",
    "\n",
    "  def train_epoch_regression(self):\n",
    "    for data, label in tqdm(self.DataLoader, leave=False,):\n",
    "      data = self.process(data)\n",
    "      mean, std = self.model(data)[1:]\n",
    "\n",
    "      loss = self.heteroskedastic_criterion(mean, std, label)\n",
    "      self.writer.add_scalar('loss/train_regression', loss, self.global_step)\n",
    "\n",
    "      self.optim.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optim.step()\n",
    "      self.global_step += 1\n",
    "  \n",
    "  @torch.no_grad()\n",
    "  def validate_classification(self):\n",
    "    avg_loss = metrics.Mean(device=device)\n",
    "    auprc = metrics.BinaryAUPRC()\n",
    "    for data, label in tqdm(self.ValLoader, leave=False,):\n",
    "      data = self.process(data)\n",
    "      pred = self.model(data)[0]\n",
    "\n",
    "      label[label != float('inf')] = 1\n",
    "      label[label == float('inf')] = 0\n",
    "      label = label.long()\n",
    "      loss = nn.CrossEntropyLoss(reduce=False)(pred, label)\n",
    "      avg_loss.update(loss)\n",
    "      auprc.update(pred[..., 1], label)\n",
    "    self.writer.add_scalar('loss/val_class', avg_loss.compute(), self.global_step)\n",
    "    self.writer.add_scalar('metrics/auprc', auprc.compute(), self.global_step)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def validate_regression(self):\n",
    "    avg_loss = metrics.Mean(device=device)\n",
    "    RMSE = metrics.Mean(device=device)\n",
    "    for data, label in tqdm(self.ValLoader, leave=False,):\n",
    "      data = self.process(data)\n",
    "      mean, std = self.model(data)[1:]\n",
    "\n",
    "      loss = self.heteroskedastic_criterion(mean, std, label)\n",
    "      avg_loss.update(loss)\n",
    "      RMSE.update((label[label != float('inf')] - mean[label != float('inf')]) ** 2)\n",
    "    self.writer.add_scalar('loss/val_class', avg_loss.compute(), self.global_step)\n",
    "    self.writer.add_scalar('metrics/RMSE', RMSE.compute() ** 0.5, self.global_step)\n",
    "\n",
    "  def heteroskedastic_criterion(self, mean, std, label, reduce=True):\n",
    "    mean = mean[label != float('inf')]\n",
    "    std = std[label != float('inf')]\n",
    "    label = label[label != float('inf')]\n",
    "    var = std ** 2\n",
    "    loss = (label - mean) ** 2 / (2 * var) + 0.5 * torch.log(2 * torch.pi * var)\n",
    "    if reduce:\n",
    "      loss = loss.mean(dim=-1)\n",
    "    return loss\n",
    "\n",
    "  def train_classification(self):\n",
    "    for _ in range(10):\n",
    "      self.train_epoch_classification()\n",
    "      self.validate_classification()\n",
    "      self.validate_regression()\n",
    "      self.writer.flush()\n",
    "  \n",
    "  def train_regression(self,):\n",
    "    for _ in range(10):\n",
    "      self.train_epoch_regression()\n",
    "      self.validate_regression()\n",
    "      self.validate_classification()\n",
    "      self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dac23dadc1e424097655e499a5fbdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550d107a9c134ceb97f46cd9f233eeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mMHTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/femibello/Documents/projects/SeizureSense/physionet.org/files/chbmit/1.0.0/chb01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mMHTrainer.__init__\u001b[0;34m(self, data_path, batch_size, learing_rate, n_fft)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mValLoader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearing_rate)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_critierion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m SummaryWriter()\n",
      "File \u001b[0;32m~/Documents/projects/SeizureSense/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m, in \u001b[0;36mMHTrainer.init_class_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, label \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDataLoader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 22\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m   pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m   label[label \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m, in \u001b[0;36mMHTrainer.process\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstft(data, n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, window\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhann_window(\u001b[38;5;241m128\u001b[39m), return_complex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     38\u001b[0m data \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(data, (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = MHTrainer('/Users/femibello/Documents/projects/SeizureSense/physionet.org/files/chbmit/1.0.0/chb01', 25, 1e-4, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac15b63143b94c2dbcefbff28b159ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = FilteredCMP(60, 10, 'tests/test_data', sop=3600, sph=0, regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/femibello/Documents/projects/SeizureSense/.venv/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".seizuresense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
